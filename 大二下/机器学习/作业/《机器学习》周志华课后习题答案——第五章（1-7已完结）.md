 

第五章课后习题答案
---------

* * *

#### 文章目录

*   [第五章课后习题答案](#_1)
*   [一、试述将线性函数f(x) = wTx用作神经元激活函数的缺陷？](#fx__wTx_5)
*   [二、试述使用图5.2(b)激活函数的神经元与对率回归的联系](#52b_9)
*   [三、对于图5.7中的Vih,试推导出BP算法中的更新公式(5.13).](#57VihBP513_17)
*   [四、试述式(5.6)中学习率的取值对神经网络训练的影响.](#56_24)
*   [五、试编程实现标准BP算法和累积BP算法,在西瓜数据集3.0上分别用这两个算法训练一一个 单隐层网络,并进行比较.](#BPBP30__27)
*   [六、试设计一个BP改进算法,能通过动态调整学习率显著提升收敛速度.编程实现该算法，并选择两个UCI数据集与标准BP算法进行实验比较.](#BPUCIBP_55)
*   [七、根据式(5.18)和(5.19)，试构造一个能解决异或问题的单层RBF神经网络.](#518519RBF_57)

一、试述将线性函数$f(x) = w^Tx$用作神经元激活函数的缺陷？
-----------------------------------------------------------------------------------------------------------------------

使用线性函数作为激活函数时，因为在单元层和隐藏层，其单元值仍是输入值X的线性组合。  
若输出层也用线性函数作为激活函数，达不到“激活”与“筛选”的目的，这样相当于整个的线性回归。  
![在这里插入图片描述](https://img-blog.csdnimg.cn/636b382928dc419a87c73478f6de09e6.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

二、试述使用图5.2(b)激活函数的神经元与对率回归的联系
-----------------------------

对率回归，是使用Sigmoid函数作为联系函数时的广义线性模型。


对于单位阶跃函数（如左图所示）：  
![请添加图片描述](https://img-blog.csdnimg.cn/136f1ba566664bafaee19c3f855e3705.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)  
单位阶跃函数不连续，难以求导，所以用对数几率函数代替。  
对于对率函数（如以上右图所示）：  
![请添加图片描述](https://img-blog.csdnimg.cn/8921a301f65540c28b570e5e4ea34318.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)  
使用Sigmoid激活函数，每个神经元几乎和对率回归相同，只不过对率回归在 \[sigmoid(x)>0.5\] 时输出为1，而神经元直接输出 \[sigmoid(x)\] 。

三、对于图5.7中的Vih,试推导出BP算法中的更新公式(5.13).
-----------------------------------

![请添加图片描述](https://img-blog.csdnimg.cn/9de2477395654907849fad91853ef658.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_19,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/1db800a2c0564d988a14080aab0846f7.png)

![请添加图片描述](https://img-blog.csdnimg.cn/be4962a0f9924a5a83e3c62edcb0c4dc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)

![请添加图片描述](https://img-blog.csdnimg.cn/e5dc4d96d70b4150922d0482384f4017.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)

四、试述式(5.6)中学习率的取值对神经网络训练的影响.
----------------------------

学习率太低，每次下降得很慢，使得迭代次数增多，各种开销增大。  
学习率太高，会在梯度下降最低点来回震荡，难以得到想要的结果。





学习率的取值对神经网络训练有着重要的影响。学习率决定了在梯度下降过程中参数更新的步长大小。不同的学习率取值会导致不同的训练动态和结果：

1. **学习率过大**：如果学习率设置得太高，可能会导致训练过程中的损失函数震荡，甚至发散，使得模型无法收敛到最优解。这是因为大步长可能会跳过最佳值，或者在最佳值附近徘徊而不是稳定下来。
2. **学习率过小**：相反，如果学习率设置得太低，虽然可以保证收敛性，但训练过程会非常缓慢。此外，过小的学习率可能导致模型陷入局部最小值，而不是全局最小值。
3. **学习率调整**：为了兼顾训练速度和模型性能，通常会采用动态调整学习率的策略。例如，可以在训练初期使用较高的学习率以快速进展，然后逐渐减小学习率以稳定训练过程并细化模型参数。
4. **自适应学习率**：有些优化算法如Adam, RMSprop等，可以自动调整学习率，这样可以在不同阶段根据模型的实际表现来调整学习步长，从而提高训练的效率和效果。

总的来说，合适的学习率取值和调整策略对于神经网络的训练至关重要，它们可以显著影响训练速度和模型的最终性能。

五、试编程实现标准BP算法和累积BP算法,在西瓜数据集3.0上分别用这两个算法训练一一个 单隐层网络,并进行比较.
---------------------------------------------------------

假设一个单隐层BP网络中，d个输入节点，隐层有q个神经元，输出层l个神经元。  
BP算法要训练的参数有  
输入层与隐层全连接的权值vijvij dq个  
隐层神经元阀值θiθi q个  
隐层与输出层全连接的权值wijwij q|个  
输出层神经元阀值yiyi l个

```
BP算法每次迭代依次计算每一个样本， 最小化该样本输出值与真实值的差距，然后将修改过参数传给下一个样本，直到达到收敛条件。这样做参数更新频繁，也可能出现参数更改相互抵销的情况，于是便有了ABP。

ABP算法每次迭代会先算出所有样本的输出,然后最小化整个样本输出与真实值的最小平方和,修改参数后进行下一-次迭代。ABP参数更新次数比BP算法少的多,但是当累计误差降到-定程度时，进-步下降会非常缓慢。
```

迭代终止条件:这里设置的终止条件是相邻一百次迭代的累计误差的差值不超过0.001.

BP算法结果:  
在西瓜数据集3上迭代1596次迭代，使得累计误差达到0.0013,此时对比表为

![请添加图片描述](https://img-blog.csdnimg.cn/3b52f86d72e543f48f57b55bd411cebf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_19,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/03efc467a33a4307a82accc42b464aaa.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_19,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/4860a792c16b4ea89236506045b217bd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_19,color_FFFFFF,t_70,g_se,x_16)  
ABP：与BP算法最大的不同是参数在计算完全部样本才更改，由于ABP后期下降很慢，所以ABP的终止条件是经过1660次迭代。累计误差达到0.0015.  
![请添加图片描述](https://img-blog.csdnimg.cn/17a39abdbcbb49a7a04f18e64a836f6f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_18,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/937aa40c7e894bf1aa973fcdd94e40cd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_19,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/c0868b1d0c064c03911ebdfa93357541.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/815cb6d3b0274271aaf66d7a30c40e83.png)  
![请添加图片描述](https://img-blog.csdnimg.cn/879db1981f5a4c1693c25576c6cf326b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)

六、试设计一个BP改进算法,能通过动态调整学习率显著提升收敛速度.编程实现该算法，并选择两个UCI数据集与标准BP算法进行实验比较.
------------------------------------------------------------------

太难了，不会。

七、根据式(5.18)和(5.19)，试构造一个能解决异或问题的单层RBF神经网络.
------------------------------------------

![请添加图片描述](https://img-blog.csdnimg.cn/040819d8b79f4b1ab0aac967c2debaca.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_17,color_FFFFFF,t_70,g_se,x_16)  
①构造数据集  
![请添加图片描述](https://img-blog.csdnimg.cn/693416c25413494f9ef6fc83df8ef38d.png)  
由此设计RBF网络  
●输入层:由于有2个输入,所以输入层2个神经元  
●隐层:隐层神经元越多拟合的越好，设为可变的t个，但至少要比输入层多1个。  
●输出层: 1个神经元.  
该网络的参数有：  
●xy:样本参数  
●wi:隐层第i个神经元与输出神经元的权值  
●ci:隐层第i个神经元的中心  
●βi:样本与第i个神经元的中心的距离的缩放系数  
Ci通过对x聚类或者随即采样获得。  
下面使用ABP算法来确定参数wi, βi;(由于是基于ABP来改的)  
![请添加图片描述](https://img-blog.csdnimg.cn/a115ae6c347c4a0da5d19c4722f8f494.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)  
迭代次数：144  
累计误差：0.000088  
结果如下：

![请添加图片描述](https://img-blog.csdnimg.cn/adb316e9a7e1410d9c2211749f0e3c80.png)  
![请添加图片描述](https://img-blog.csdnimg.cn/f697c6f308c24ba3a8b38f26236e1763.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)  
![请添加图片描述](https://img-blog.csdnimg.cn/09bc83e091fa4d9b8bdeefce61393e56.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_20,color_FFFFFF,t_70,g_se,x_16)

 

文章知识点与官方知识档案匹配，可进一步学习相关知识

[OpenCV技能树](https://edu.csdn.net/skill/opencv/?utm_source=csdn_ai_skill_tree_blog)[首页](https://edu.csdn.net/skill/opencv/?utm_source=csdn_ai_skill_tree_blog)[概览](https://edu.csdn.net/skill/opencv/?utm_source=csdn_ai_skill_tree_blog)25460 人正在系统学习中

本文转自 <https://blog.csdn.net/weixin_45626630/article/details/121087332>，如有侵权，请联系删除。