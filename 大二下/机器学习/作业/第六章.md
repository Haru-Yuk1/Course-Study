18/11/23更新

**6.11 自己编程实现SVM，并在西瓜数据集 3.0**αα **测试。**

**答：**

**自己加的题目。虽然书上没要求，但还是自己写了一遍。**

代码在：

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch6--%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA/mySVM)

其实主要就是把SMO实现了一遍。

参考：

*   《统计学习方法》
*   《机器学习实战》
*   《Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines》
*   《THE IMPLEMENTATION OF SUPPORT VECTOR MACHINES USING THE SEQUENTIAL MINIMAL OPTIMIZATION ALGORITHM》

写代码的时候参考以上资料。代码主要根据原论文中提供的伪代码和《机器学习实战》的源码写的，《机器学习实战》中给出的代码和原论文有几个地方有差异：

1.  在选择第二个 α\\alpha\\alpha 变量时，原论文给出的方法是1、首先从间隔边界上的支持向量( 0<α<C0<\\alpha<C0<\\alpha<C )中，找到使得 |Ei−Ej|\\lvert{E\_{i}-E\_{j}}\\rvert\\lvert{E\_{i}-E\_{j}}\\rvert 最大的 α\\alpha\\alpha 。2、若上面选的 α\\alpha\\alpha 不行，则遍历所有的支持向量。3、还是不行则遍历所有的样本点。4、若所有样本点都不行，则放弃一个变量。关于认定 α\\alpha\\alpha 不行的原则，原论文描述的是：不能使得目标函数有足够下降。实际上在伪代码中是使得 α\\alpha\\alpha 本身有足够下降就认为不行。而《机器学习实战》中代码从更新过的 α\\alpha\\alpha 中选择使得 |Ei−Ej|\\lvert{E\_{i}-E\_{j}}\\rvert\\lvert{E\_{i}-E\_{j}}\\rvert 最大的 α\\alpha\\alpha作为第二个变量。若不行则直接放弃第一个变量。不知道这一点是改进还是简化。代码中是按照论文的方式实现的
2.  为了选择第二个变量时方便，SMO会将所有支持向量的误差 EEE 建立缓存，且在每次更新完 α\\alpha\\alpha 之后，都同时更新误差缓存。《机器学习实战》源码中，在给支持向量建立误差缓存时，虽然有更新 EkE\_{k}E\_{k} 的步骤，但只更新了每次更新的两个变量 αi,αj\\alpha\_{i},\\alpha\_{j}\\alpha\_{i},\\alpha\_{j} 对应的误差，并且更新之后也没有使用，在选择第二个变量计算 |Ei−Ej|\\lvert{E\_{i}-E\_{j}}\\rvert\\lvert{E\_{i}-E\_{j}}\\rvert 时，都重新计算了 EEE 。在自己实现的时候，这一点也是按照论文来的。
3.  最后一点，在更新 aaa 时，需要计算 η\=K11+K22−2K12\\eta=K\_{11}+K\_{22}-2K\_{12}\\eta=K\_{11}+K\_{22}-2K\_{12} （参考统计学习方法 p127 式7.107），在极少数情况下 η\\eta\\eta 会等于零，在原论文中给出了针对这种情况下 aaa 的更新方式，在《机器学习实战》中，这种情况会直接跳过更新。这里代码和《机器学习实战》一致，直接跳过了，这一点其实影响不大。

另外实际上更新误差缓存 EEE 有更高效的方法，是在第四个参考文献里面发现的。不过在代码里面没有实现。因为感觉有点复杂。。。有兴趣的可以看看那篇论文，在3.4小节解释了更新误差缓存的方式。

最后自己写的代码在西瓜数据集3.0α上测试了一下，训练后决策边界如下：

<img src="https://pic2.zhimg.com/v2-336bf17e755c8d045fd192c3c1f2dd91\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-336bf17e755c8d045fd192c3c1f2dd91\_r.jpg"/>

![](https://pic2.zhimg.com/80/v2-336bf17e755c8d045fd192c3c1f2dd91_1440w.webp)

训练结果和使用sklearn中结果（习题6.2）一致，支持向量也是相同的，决策边界差不多相同，其他数据未测试。不过速度上，sklearn会快很多，测试了一下训练西瓜数据集，自己写的代码需要5e-4秒，而sklearn只需要1e-8。还是有很大差距的。

代码有点乱，这里只为深刻理解一下SMO，也不做工程使用，暂时就不优化了。。以后闲下来再看看。

以上。

* * *

**6.1 试证明样本空间中任意点 xxx 到超平面 (w,b)(w,b)(w,b) 的的距离为式 (6.2)。**

**答：**
$$
图中，令A点到超平面（点B）的距离为\gamma，于是 \bar{BA}=\gamma*\frac{w}{\left| w \right|}\bar{BA}=\gamma*\frac{w}{\left| w \right|}\\ ( w|w| 是 w 同向的单位向量， 对于超平面(w,b)(w,b)(w,b) 其垂直方向即 www )，对于B点有： wTB¯+b\=0w^{T}\\bar{B} +b=0w^{T}\\bar{B} +b=0 ，而 B¯\=A¯−BA¯\\bar{B}= \\bar{A}-\\bar{BA}\\bar{B}= \\bar{A}-\\bar{BA} ，于是 wT(A¯−γ∗w|w|)+b\=0w^{T}(\\bar{A}-\\gamma\*\\frac{w}{\\left| w \\right|}) +b=0w^{T}(\\bar{A}-\\gamma\*\\frac{w}{\\left| w \\right|}) +b=0 ，可得 wT∗A¯−γ∗|w|+b\=0⇒γ\=wTA¯+b|w|w^{T}\*\\bar{A}-\\gamma\*\\left| w \\right|+b=0\\Rightarrow\\gamma=\\frac{w^{T}\\bar{A}+b}{\\left| w \\right|}w^{T}\*\\bar{A}-\\gamma\*\\left| w \\right|+b=0\\Rightarrow\\gamma=\\frac{w^{T}\\bar{A}+b}{\\left| w \\right|} ，这里的 A¯\\bar{A}\\bar{A} 即书中 xxx ，即可的式6.2。
$$
<img src="https://pic4.zhimg.com/v2-4092739fd9556bc5cf93fa798ae8b29f\_b.jpg" data-caption="" data-size="small" data-rawwidth="315" data-rawheight="284" class="content\_image" width="315"/>

![](https://pic4.zhimg.com/80/v2-4092739fd9556bc5cf93fa798ae8b29f_1440w.webp)

这个问题在吴恩达老师的《机器学习》课程（斯坦福版本CS 229）里面讲解过，有兴趣的可以自己去网易公开课看看，图片截图自该课程课件。

  

**6.2 试使用 LIBSVM，在西瓜数据集 3.0α 上分别用线性核和高斯核训练一个 SVM，并比较其支持向量的差别。**

**答：**

这里没用LIBSVM，用的sklearn中的sklearn.svm.svc，它的实现也是基于libsvm的。

**使用不同参数的时候，支持向量是不同的（没有对高斯核中的gamma调参）。**由于西瓜数据集3.0a线性不可分，所以使用线性核时，无论惩罚系数多高 ，还是会出现误分类的情况；而使用高斯核时在惩罚系数设置较大时，是可以完全拟合训练数据。所以在惩罚系数设置较小时，两者支持向量都类似，而在惩罚系数较大（支持向量机中，惩罚系数越大，正则化程度越低）时，高斯核的支持向量数目会较少，而线性核的会几乎没有变化。

代码在：[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch6--%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA/6.2)

C = 100时训练情况如下：

高斯核支持向量： \[ 8 9 11 12 13 14 16 2 3 4 5 6 7\]

<img src="https://pic4.zhimg.com/v2-b7de878ee2a79ed1a79980a1841539a3\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-b7de878ee2a79ed1a79980a1841539a3\_r.jpg"/>

![](https://pic4.zhimg.com/80/v2-b7de878ee2a79ed1a79980a1841539a3_1440w.webp)

\----------------------------------------

线性核支持向量： \[ 8 9 11 12 13 14 16 2 3 4 5 6 7\]

<img src="https://pic1.zhimg.com/v2-c81c1eb3dcc72a90936623e6b2dd1ff8\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-c81c1eb3dcc72a90936623e6b2dd1ff8\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

* * *

C = 10000时训练情况如下：

高斯核支持向量： \[11 13 14 1 5 6\]

<img src="https://pic4.zhimg.com/v2-8e22b79f0f2d31c780bd3c18ad301cf3\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-8e22b79f0f2d31c780bd3c18ad301cf3\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

\----------------------------------------

线性核支持向量： \[ 9 11 12 13 14 16 2 3 4 5 6 7\]

<img src="https://pic1.zhimg.com/v2-cb8908491219d290a8d538516b959f40\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-cb8908491219d290a8d538516b959f40\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

  

**6.3 选择两个 UCI 数据集，分别用线性核和高斯核训练一个 SVM，并与BP 神经网络和 C4.5 决策树进行实验比较。**

**答：**

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch6--%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA/6.3)

  

**6.4 试讨论线性判别分析与线性核支持向量机在何种条件下等价。**

**答：**



线性判别分析（LDA）与线性核支持向量机（SVM）在某些条件下可以是等价的。具体来说，当数据线性可分，并且问题是二分类问题时，这两种方法可能会产生相同的决策边界。这意味着，如果我们将数据映射到高维空间，并且在这个空间中数据是线性可分的，那么LDA和线性核SVM可能会得到相同的结果。

在LDA中，目标是最大化类间散度矩阵与类内散度矩阵的比率，从而找到一个投影方向，使得不同类别的数据在这个方向上有最大的区分度。而线性核SVM的目标是找到一个超平面，使得不同类别的数据之间的间隔最大化。

当数据满足以下条件时，LDA和线性核SVM可能等价：

1. 数据集是线性可分的。
2. 问题是二分类问题。
3. 类别内的数据是高斯分布，且具有相同的协方差矩阵。

在实际应用中，这些条件可能不会完全满足，因此LDA和线性核SVM通常会产生不同的结果。但在理论上，当上述条件满足时，两者可以产生相同的分类决策函数。



**这道题想不出很明确的答案，这仅讨论一下。**

有一点很明确的是：在数据线性可分时并不会导致线性判别分析与线性核支持向量机等价。

<img src="https://pic4.zhimg.com/v2-970fe96fafdfa66080d5ef1c076fd76f\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-970fe96fafdfa66080d5ef1c076fd76f\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

上图是以iris数据中第0列和第2列数据作为训练集。分别LDA和线性核SVM训练，得到图中两者的决策边界，可以看出在数据线性可分的情况下，两者决策边界还是有很大差别的。如果这里等价理解为两者决策边界相同，即两个模型决策函数是相同的，那么两者决策边界重合时，两者等价的。那么什么时候两者会重叠？

事实上，可以想象到，LDA的决策边界的斜率可以由投影面 ww 得到，其斜率是垂直于 ww 的，而截距则可由两类样本的中心点 在 ww 投影 w^{T}u\_{0},w^{T}u\_{1}w^{T}u\_{0},w^{T}u\_{1} 得到，即LDA决策边界通过 w^{T}u\_{0},w^{T}u\_{1}w^{T}u\_{0},w^{T}u\_{1} 的中点（公式参考原书p60）。

而线性核SVM的决策边界则由模型参数 w\_{svm},bw\_{svm},b 得到（对应原书式6.12），所以当SVM中的参数 w\_{svm}w\_{svm} 和LDA中投影面 w\_{lda}w\_{lda} 垂直，且 SVM的w\_{svm},bw\_{svm},b 通过两类样本中心在 w\_{lda}w\_{lda} 的投影的中点时，两者等价。只讨论到这里了。

查了很多资料没找到相关信息。感觉LDA和SVM其实没有多大相似度。

ps.这里解答其实就按照结果倒推了一下。貌似都是些废话。

画图代码在：[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch6--%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA/6.4)

代码有点乱。

  

**6.5 试述高斯核 SVM 与 RBF 神经网络之间的联系。**

**答：**





高斯核支持向量机（SVM）与径向基函数（RBF）神经网络之间的主要联系在于它们都使用了径向基函数，特别是高斯核函数，来处理非线性特征的映射。高斯核函数在这两种方法中起到了类似的作用，它们都利用这个核函数将原始输入空间映射到一个更高维的特征空间，在这个空间中，非线性可分的数据可能变得线性可分。

在高斯核 SVM 中，高斯核函数用于计算输入样本与支持向量之间的相似度，这有助于找到一个最优的超平面，以最大化分类边界的间隔。而在 RBF 神经网络中，高斯核函数用作隐层神经元的激活函数，其中心点可以是训练样本中的抽样或者是多个聚类中心，这样的网络结构使得网络能够捕捉到数据中的局部特征。

如果在 RBF 神经网络中，每个训练样本都对应一个神经元中心，并且隐层神经元的数量设置为训练样本的数量，那么这样的 RBF 神经网络与使用高斯核函数的 SVM 在预测函数上是完全等价的。这是因为在这种情况下，两者都使用了相同的核函数来进行特征映射，并且都在高维特征空间中寻找最优的决策边界

支持向量机（SVM）对噪声敏感的原因主要是因为它是基于边界的分类算法。在SVM中，分类的决策边界是由距离最近的样本点，即支持向量，确定的。因此，如果训练数据中包含噪声点，这些噪声点可能成为支持向量，从而影响决策边界的位置。这可能导致边界偏移或错误分类，特别是在硬间隔SVM中，它要求所有样本都满足硬间隔约束，即函数间隔要大于1，这使得模型对噪声尤其敏感。

为了克服这个问题，可以使用软间隔SVM，它通过引入松弛变量和惩罚因子C来允许一定程度的误分类。这样，模型可以在保持较大间隔的同时，对噪声点更加鲁棒。松弛变量表示样本离群的程度，而惩罚因子C则表示对这些离群点的容忍度。选择合适的C值可以在模型复杂度和训练误差之间取得平衡，从而提高模型对噪声的鲁棒性。



其实这个题目在p145的《休息一会儿》的注释里面已经给出答案了。

RBF神经网络中，将隐藏层神经元个数设置为训练样本数，每个样本设置为一个神经元中心，此时RBF的预测函数和SVM激活函数相同。

个人理解，两个模型还是有挺大差别的。

*   RBF中径向基激活函数中控制方差的参数 \\beta\\beta 是由模型自动习得，而在SVM中是一个可调节的超参。
*   目标函数、优化方式也不同。

但是如果将RBF中 \\beta\\beta 固定为和SVM一致，最后训练结果应该会比较相似。

以上是个人理解。就不写代码验证了。。。

  

**6.6 试析 SVM 对噪声敏感的原因。**

**答：**

SVM的决策只基于少量的支持向量，若噪音样本出现在支持向量中，容易对决策造成影响，所以SVM对噪音敏感。

  

**6.7 试给出式 (6.52) 的完整 KKT 条件。**

**答：**

6.52式是经过将完整的KKT条件

\\nabla\_{w}L =0,\\nabla\_{b}L =0,\\nabla\_{\\xi\_{i}}L =0,\\nabla\_{\\hat{\\xi\_{i}}}L =0\\nabla\_{w}L =0,\\nabla\_{b}L =0,\\nabla\_{\\xi\_{i}}L =0,\\nabla\_{\\hat{\\xi\_{i}}}L =0 ，这里对应着原书式6.47-6.50合并之后的。完整的如下：

\\alpha\_{i}(f(x\_{i})-y\_{i}-\\epsilon-\\xi\_{i})=0,a\_{i}\\geq0,f(x\_{i})-y\_{i}-\\epsilon-\\xi\_{i}\\leq0\\alpha\_{i}(f(x\_{i})-y\_{i}-\\epsilon-\\xi\_{i})=0,a\_{i}\\geq0,f(x\_{i})-y\_{i}-\\epsilon-\\xi\_{i}\\leq0

\\hat\\alpha\_{i}(y\_{i}-f(x\_{i})-\\epsilon-\\hat{\\xi\_{i}})=0,\\hat{a\_{i}}\\geq0,y\_{i}-f(x\_{i})-\\epsilon-\\hat{\\xi\_{i}}\\leq0\\hat\\alpha\_{i}(y\_{i}-f(x\_{i})-\\epsilon-\\hat{\\xi\_{i}})=0,\\hat{a\_{i}}\\geq0,y\_{i}-f(x\_{i})-\\epsilon-\\hat{\\xi\_{i}}\\leq0

u\_{i}\\xi\_{i}=0, u\_{i}\\geq0, -\\xi\_{i}\\leq0u\_{i}\\xi\_{i}=0, u\_{i}\\geq0, -\\xi\_{i}\\leq0

\\hat{u\_{i}}\\hat{\\xi\_{i}}=0, \\hat{u\_{i}}\\geq0, -\\hat{\\xi\_{i}}\\leq0\\hat{u\_{i}}\\hat{\\xi\_{i}}=0, \\hat{u\_{i}}\\geq0, -\\hat{\\xi\_{i}}\\leq0

6.52中其他公式的推导出来的。

  

**6.8 以西瓜数据集 3.0α 的"密度"为输入"含糖率"为输出，试使用LIBSVM 训练一个 SVR。**

**答：**

关于SVR没有理解很深，简单了解了一下。这道题就简单看一下不同参数，训练结果的变换吧。

<img src="https://pic3.zhimg.com/v2-ff079fc5b57a13fe14d4478cecbaa5ee\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic3.zhimg.com/v2-ff079fc5b57a13fe14d4478cecbaa5ee\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

<img src="https://pic2.zhimg.com/v2-e03fd3838cae9d6b428a0e1b41991ff9\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-e03fd3838cae9d6b428a0e1b41991ff9\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='473'></svg>)

直观上看，含糖率和密度无明显关系。所以无论模型参数怎么调，看上去对数据的拟合都不是很好，预测值和真实值还是有较大差异。不过还是可以看出来随着gamma或者C的增大，模型都会趋于更加复杂。

这里代码很简单，还是放上来。

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSet](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch6--%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA/6.8)

  

**6.9 试使用核技巧推广对率回归，产生"核对率回归"。**

**答：**

对于 w^{T}x+bw^{T}x+b 形式的模型，即线性模型，使用核技巧的关键点在于最优的 w^{\*}w^{\*} 可以由训练集的线性组合表示，即 w^{\*}=\\sum\_{i}\\beta\_{i}x\_{i}w^{\*}=\\sum\_{i}\\beta\_{i}x\_{i} ，使得模型可表示为 \\sum\_{i}\\beta\_{i}\\langle x\_{i},x\\rangle+b\\sum\_{i}\\beta\_{i}\\langle x\_{i},x\\rangle+b ，进而使用核函数直接计算数据点在高维空间内积，而不显式的计算数据点从低维到高维的映射。

原命题：事实上对于任何L2正则化的线性模型： \\min\_{w} \\frac{\\lambda}{N}w^{T}w+\\frac{1}{N}\\sum\_{n=1}^{N}err(y\_{n},w^{T}x\_{n})\\min\_{w} \\frac{\\lambda}{N}w^{T}w+\\frac{1}{N}\\sum\_{n=1}^{N}err(y\_{n},w^{T}x\_{n}) ，这里，其最优值都可以表示为 w^{\*}=\\sum\_{i}\\beta\_{i}x\_{i}w^{\*}=\\sum\_{i}\\beta\_{i}x\_{i} 。其证明参考下图：（截图自林轩田讲授的《机器学习技法》课程第五章课件）

<img src="https://pic3.zhimg.com/v2-939584402c6c44ce1a29ce6992abd82a\_b.jpg" data-caption="" data-size="normal" data-rawwidth="732" data-rawheight="548" class="origin\_image zh-lightbox-thumb" width="732" data-original="https://pic3.zhimg.com/v2-939584402c6c44ce1a29ce6992abd82a\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='732' height='548'></svg>)

（上图中 z\_{n}z\_{n} 可以理解为数据点 x\_{n}x\_{n} 或者 x\_{n}x\_{n} 在高维空间的映射 \\phi(x\_{n})\\phi(x\_{n}) ）

上图通过反证法来证明：

将 w\_{\*}w\_{\*} 分解为与 z\_{n}z\_{n} 空间平行的 w\_{\\lVert}w\_{\\lVert} 和垂直的 w\_{\\bot}w\_{\\bot} ，若 w\_{\\bot}=0w\_{\\bot}=0 则表示 w\_{\*}w\_{\*} 可以表示为 z\_{n}z\_{n} 的线性组合。

假设 w\_{\*}w\_{\*} 为最优解且 w\_{\\bot}\\ne0w\_{\\bot}\\ne0 。由于 w\_{\\bot}w\_{\\bot} 与 z\_{n}z\_{n} 空间垂直，于是 w\_{\\bot}^{T}z\_{n}=0w\_{\\bot}^{T}z\_{n}=0 ， 因此w\_{\\bot}w\_{\\bot} 不会对目标函数中 errerr 项的大小产生影响，而对于 w\_{\*}^{T}w\_{\*}w\_{\*}^{T}w\_{\*} ，在 w\_{\\bot}\\ne0w\_{\\bot}\\ne0 的情况下必定有： w\_{\*}^{T}w\_{\*}>w\_{\\lVert}^{T}w\_{\\lVert}w\_{\*}^{T}w\_{\*}>w\_{\\lVert}^{T}w\_{\\lVert} ，显然 w\_{\\lVert}w\_{\\lVert} 比 w\_{\*}w\_{\*} “更优”，即 w\_{\*}w\_{\*} 不是最优解。于是原命题得证。

那么对于L2正则化的逻辑回归，其核形式即如下图：

<img src="https://pic4.zhimg.com/v2-e8175311573c162e3423cf9ff57f560f\_b.jpg" data-caption="" data-size="normal" data-rawwidth="725" data-rawheight="544" class="origin\_image zh-lightbox-thumb" width="725" data-original="https://pic4.zhimg.com/v2-e8175311573c162e3423cf9ff57f560f\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='725' height='544'></svg>)

可直接使用梯度下降等优化算法求解上图的目标函数即

题目没要求。就偷个懒不写代码了。

说个题外的。SVM中只有少数支持向量对应的 a\_{i}a\_{i} 非零，所以对于SVM来说，训练完成后只需要储存个别非零的 a\_{i}a\_{i} 和对应数据点即可；而不同于SVM， 核逻辑回归并没有这一性质，需要储存所有训练数据。就这一点来说核逻辑回归并不高效。

  

**6.10\* 试设计一个能显著减少 SVM 中支持向量的数目而不显著降低泛化性能的方法。（未完成）**

**答：**

这个应该也是某个论文。最近时间不多，暂时不深究了就。。

本文转自 <https://zhuanlan.zhihu.com/p/49023182?utm_id=0>，如有侵权，请联系删除。