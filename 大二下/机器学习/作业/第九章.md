**9.1 试证明 : p≥1p \\geq1p \\geq1 时，**闵可夫斯基**距离满足距离度量的四条基本性质； 0≤p<10\\leq p < 10\\leq p < 1 时，闵可夫斯基距离不满足直递性，但满足非负性、同一性、对称性；P 趋向无穷大时，**闵可夫斯基距离**等于对应分量的最大绝对距离，即**

limp→+∞(∑u\=1n|xiu−xju|p)1p\=maxu|xiu−xju|\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}=\\max\_u\\left| x\_{iu}-x\_{ju} \\right|\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}=\\max\_u\\left| x\_{iu}-x\_{ju} \\right| .


$$
lim_{p \rightarrow +\infty}{(\sum_{u=1}^n | x_{iu} -x_{ju} |^p)^{\frac{1}{p}}}=\max_u| x_{iu}-x_{ju} 
$$
**答：**

非负性、同一性、对称性很显然，关键是直递性了，关于直递性就是闵可夫斯基不等式的证明，具体参考：闵可夫斯基不等式。

关于闵可夫斯基距离，令 a\=maxu|xiu−xju|a =\\max\_u\\left| x\_{iu}-x\_{ju} \\right|a =\\max\_u\\left| x\_{iu}-x\_{ju} \\right| ，那么 limp→+∞(∑u\=1n|xiu−xju|p)1p≥limp→+∞(ap)1p\=a\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}\\geq\\lim\_{p \\rightarrow +\\infty}(a^p)^{\\frac{1}{p}}=a\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}\\geq\\lim\_{p \\rightarrow +\\infty}(a^p)^{\\frac{1}{p}}=a

limp→+∞(∑u\=1n|xiu−xju|p)1p≤limp→+∞(nap)1p\=a\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}\\leq\\lim\_{p \\rightarrow +\\infty}(na^p)^{\\frac{1}{p}}=a\\lim\_{p \\rightarrow +\\infty}{(\\sum\_{u=1}^n\\left| x\_{iu} -x\_{ju} \\right|^p)^{\\frac{1}{p}}}\\leq\\lim\_{p \\rightarrow +\\infty}(na^p)^{\\frac{1}{p}}=a .于是得证。

  

**9.2 同一样本空间中的集合 X 与 Z 之间的距离可通过"豪斯多夫距离" (Hausdorff distance)计算：**

**distH(X,Z)\=max(dishh(X,Z),disth(Z,X))dist\_H(X,Z)=\\max(dish\_h(X,Z),dist\_h(Z,X))dist\_H(X,Z)=\\max(dish\_h(X,Z),dist\_h(Z,X)) ,**

**其中 disth(X,Z)\=maxx∈Xminz∈Z‖x−z‖2dist\_h(X,Z)=\\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2dist\_h(X,Z)=\\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2 .**

**试证明:豪斯多夫距离满足距离度量的四条基本性质.**

**答：**

*   **非负：** disth(X,Z)\=maxx∈Xminz∈Z‖x−z‖2≥0dist\_h(X,Z)=\\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2\\geq0dist\_h(X,Z)=\\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2\\geq0 ，所以 distH(X,Z)≥0dist\_H(X,Z)\\geq0dist\_H(X,Z)\\geq0 ；
*   **同一性：** 若 X≠ZX\\ne ZX\\ne Z ，不失一般性，假设 xi≠zix\_i\\ne z\_ix\_i\\ne z\_i ，其他的样本都完全相同，那么对于 xj,j\=1,2,..i−1,i,...,nx\_j,j=1, 2,..i-1, i,...,nx\_j,j=1, 2,..i-1, i,...,n 都有 zjz\_jz\_j 使得 minz∈Z‖xj−z‖2\=0\\min\_{z\\in Z}\\Vert x\_j-z\\Vert\_2=0\\min\_{z\\in Z}\\Vert x\_j-z\\Vert\_2=0 ，而对于 xix\_ix\_i ，由于没有相同的样本，所以 minz∈Z‖xi−z‖2\>0⇒maxx∈Xminz∈Z‖x−z‖2\>0\\min\_{z\\in Z}\\Vert x\_i-z\\Vert\_2>0\\Rightarrow \\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2>0\\min\_{z\\in Z}\\Vert x\_i-z\\Vert\_2>0\\Rightarrow \\max\_{x\\in X}\\min\_{z\\in Z}\\Vert x-z\\Vert\_2>0 。原命题得证；
*   **对称性：** ，distH(X,Z)\=max(dishh(X,Z),disth(Z,X))\=max(disth(Z,X)，dishh(X,Z))\=distH(Z,X)dist\_H(X,Z)=\\max(dish\_h(X,Z),dist\_h(Z,X))=\\max(dist\_h(Z,X)，dish\_h(X,Z))=dist\_H(Z,X)dist\_H(X,Z)=\\max(dish\_h(X,Z),dist\_h(Z,X))=\\max(dist\_h(Z,X)，dish\_h(X,Z))=dist\_H(Z,X)
*   **直递性：**太难了。不会。

  

**9.3 试析 k 均值算法能否找到最小化式 (9.24) 的最优解.**

**答：**

不能，因为 k 均值本身是 NP 问题，且 9.24 是非凸的（具体证明不太懂.），容易陷入局部最优是 k 均值的一个缺点吧，所以在使用 k 均值时常常多次随机初始化中心点，然后挑选结果最好的一个。


**不能**。k 均值算法本身是一个NP难题，而且式 (9.24) 是非凸的。这意味着算法容易陷入局部最优解，而不是全局最优解。因此，在实际应用中，通常会采用多次随机初始化中心点的方法，然后选择最好的结果。



**9.4 试编程实现 k 均值算法，设置三组不同的 k 值、三组不同初始中心点，在西瓜数据集 4.0 上进行实验比较，并讨论什么样的初始中心有利于取得好结果.**

**答：**

代码在：[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch9--%25E8%2581%259A%25E7%25B1%25BB) ，暂时先不分析初始化点和结果了。

  

**9.5 基于 DBSCAN 的概念定义，若 x 为核心对象，由 x 密度可达的所有样本构成的集合为 X. 试证明 :X 满足连接性 (9.39)与最大性 (9.40).**

**答：**

*   **连接性：** 由于任意 x^{'} \\in Dx^{'} \\in D 都由 xx 密度可达，于是任意 x\_i,x\_j\\in Dx\_i,x\_j\\in D 都可通过 xx 密度相连；
*   **最大性：** x\_i \\in D\\Rightarrowx\_i \\in D\\Rightarrow x\_ix\_i 由 xx 密度可达， x\_jx\_j 由 x\_ix\_i 密度可达 \\Rightarrow\\Rightarrow x\_jx\_j 由 xx 密度可达 \\Rightarrow x\_j \\in D\\Rightarrow x\_j \\in D 。

  

**9.6 试析 AGNES 算法使用最小距离和最大距离的区别.**

**答：**

个人理解，不一定正确。使用最小距离合并聚类簇时，最终聚类结果趋于不同类别之间的“空隙”会更大；而最大距离约等于最小距离加上两个类别的离散程度，这里离散程度可理解为方差，方差越大，两个类别的最大距离越大，所以使用最大距离时，会尽量使得类别的方差尽量小，最终聚类结果也趋于类内更集中。

其实类似于线性判别分析中类内方差尽量小，类间距离尽量大。

  AGNES算法是一种层次聚类算法，它通过计算类间的距离来决定合并哪些类。在AGNES算法中，使用最小距离（Single Linkage）和最大距离（Complete Linkage）是两种不同的类间距离计算方法，它们对聚类结果有显著影响。

- **最小距离**：这种方法也称为单连接（Single Linkage），它定义两个类之间的距离为这两个类中最近的两个样本之间的距离。这种方法倾向于产生较长且细的簇，容易受到噪声和异常点的影响，可能导致所谓的“链效应”，即不相似的对象通过一系列相似的对象被错误地归入同一类。
- **最大距离**：又称为全连接（Complete Linkage），它定义两个类之间的距离为这两个类中最远的两个样本之间的距离。这种方法倾向于产生更紧凑且均匀的簇，对噪声和异常点的影响较小，但可能导致簇的分离度不够，尤其是当簇的实际形状远离球形时。

总的来说，最小距离方法更倾向于发现非球形的簇和结构，而最大距离方法则倾向于发现紧凑的、接近球形的簇。选择哪种方法取决于数据的特性以及聚类的目标)。

**9.7 聚类结果中若每个簇都有一个凸包(包含簇样本的凸多面体) ，且这些凸包不相交，则称为凸聚类.试析本章介绍的哪些聚类算法只能产生凸聚类，哪些能产生非凸聚类.**

**答：**

若在一个簇的凸包之内，有其他簇的样本，就说明凸包相交。

*   **原型聚类：**输出线性分类边界的聚类算法显然都是凸聚类，这样的算法有：K均值，LVQ；而曲线分类边界的也显然是非凸聚类，高斯混合聚类，在簇间方差不同时，其决策边界为弧线，所以高混合聚类为非凸聚类；
*   **密度聚类：**DBSCAN，如下图情况，显然当领域参数符合一定条件时，会生成两个簇，其中外簇会包括内簇，所以DBSCAN显然也是非凸聚类；

<img src="https://pic3.zhimg.com/v2-3212f7a037021e3254c1beeb75d36a06\_b.jpg" data-caption="" data-size="normal" data-rawwidth="800" data-rawheight="797" class="origin\_image zh-lightbox-thumb" width="800" data-original="https://pic3.zhimg.com/v2-3212f7a037021e3254c1beeb75d36a06\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='800' height='797'></svg>)

*   **层次聚类：**AGENS，这个暂时没想明白怎么分析。从书中给出的示例，是凸聚类。

  

**9.8 试设计一个聚类性能度量指标，并与 9.2 节中的指标比较.**

**答：**

**参考线性判别分析的优化目标：同类协方差尽量小，异类中心之间距离尽量大。**

  

**9.9\* 试设计一个能用于混合属性的非度量距离.**

**答：**

样本 x\_i,x\_jx\_i,x\_j 的距离为： d(x\_i,x\_j)=\\frac{\\sum\_{n=1}^{N}\\delta^n\_{ij}d^n\_{ij}}{\\sum\_{n=1}^{N}\\delta^n\_{ij}}d(x\_i,x\_j)=\\frac{\\sum\_{n=1}^{N}\\delta^n\_{ij}d^n\_{ij}}{\\sum\_{n=1}^{N}\\delta^n\_{ij}} ，其中当 x\_{in},x\_{jn}x\_{in},x\_{jn} 缺失时， \\delta^n\_{ij}=0\\delta^n\_{ij}=0 ，其他为1；

*   当前属性 nn 为数值类型时， d^n\_{ij} =\\frac{\\left| x\_{in}-x\_{jn} \\right|}{\\max(X)-\\min(X)}d^n\_{ij} =\\frac{\\left| x\_{in}-x\_{jn} \\right|}{\\max(X)-\\min(X)} ；
*   当属性 nn 为类别型或二元型时， x\_{in}=x\_{jn}x\_{in}=x\_{jn} 时， d\_{ij}^n=1d\_{ij}^n=1 ，否则为0；
*   当前属性 nn 为序数型时，即 x\_{in}\\in\[1, 2,...,M\_n\]x\_{in}\\in\[1, 2,...,M\_n\] ，先将其归一化， z\_{in} = \\frac {x\_{in}-1}{M\_n -1}z\_{in} = \\frac {x\_{in}-1}{M\_n -1} ，然后将 z\_{in}z\_{in} 作为数值属性来处理。

这里的计算其实很简单，就是把连续属性归一化；而离散属性有序时则归一化话再按照连续属性处理，无序时则相等为1，不等为0.

参考：《数据挖掘概念与技术》.韩家炜，2.4节.

  

**9.10\* 试设计一个能自动确定聚类数的改进 k 均值算法，编程实现并在西瓜数据集 4.0 上运行.**

**答：**

**待补充**

本文转自 <https://zhuanlan.zhihu.com/p/59385748>，如有侵权，请联系删除。