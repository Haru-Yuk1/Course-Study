**7.1 试使用极大似然法估算回瓜数据集 3.0 中前 3 个属性的类条件概率.**

**答：**

![img](%E7%AC%AC%E4%B8%83%E7%AB%A0.assets/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBATWVzc29yMjAyMA==,size_13,color_FFFFFF,t_70,g_se,x_16.png)

以第一个属性色泽为例，其值计数如下：

*   色泽 乌黑 浅白 青绿
*   好瓜
*   否 2 4 3
*   是 4 1 3

令 乌黑是p乌黑|是\=p1p\_{乌黑|是}=p\_{1}p\_{乌黑|是}=p\_{1} 表示好瓜中色泽为“乌黑”的概率， 浅白是p浅白|是\=p2p\_{浅白|是}=p\_{2}p\_{浅白|是}=p\_{2} 为好瓜中“浅白”的概率， 青绿是p青绿|是\=p3p\_{青绿|是}=p\_{3}p\_{青绿|是}=p\_{3} ， p3\=1−p2−p3p\_{3} =1-p\_{2}-p\_{3}p\_{3} =1-p\_{2}-p\_{3} ， 是D是D\_{是}D\_{是} 表示好瓜的样本，其他类同，于是色泽属性的似然概率则可表示为色泽是是L(p)\=P(X色泽|Y\=是)\=∏x∈D是P(x)\=p14p21(1−p1−p2)3L(p)=P(X\_{色泽}|Y=是)=\\prod\_{x\\in D\_{是}}P(x)=p\_{1}^{4}p\_{2}^{1}(1-p\_1-p\_{2})^{3}L(p)=P(X\_{色泽}|Y=是)=\\prod\_{x\\in D\_{是}}P(x)=p\_{1}^{4}p\_{2}^{1}(1-p\_1-p\_{2})^{3} ,其对数似然为： LL(p)\=4ln(p1)+ln(p2)+3ln(1−p1−p2)LL(p)=4ln(p\_1)+ln(p\_2)+3ln(1-p\_1-p\_2)LL(p)=4ln(p\_1)+ln(p\_2)+3ln(1-p\_1-p\_2) ，分别对 p1,p2p\_1,p\_2p\_1,p\_2 求偏导并使其为零，即可得 p1,p2p\_1,p\_2p\_1,p\_2 的极大似然估计： p1^\=12,p2^\=18,p^3\=1−p^2−p^3\=38\\hat{p\_1}=\\frac{1}{2},\\hat{p\_2}=\\frac{1}{8},\\hat p\_{3} =1-\\hat p\_{2}-\\hat p\_{3}=\\frac{3}{8}\\hat{p\_1}=\\frac{1}{2},\\hat{p\_2}=\\frac{1}{8},\\hat p\_{3} =1-\\hat p\_{2}-\\hat p\_{3}=\\frac{3}{8} ，同理可得 色泽否P(X色泽|Y\=否)P(X\_{色泽}|Y=否)P(X\_{色泽}|Y=否) 的似然概率，进而求得类为“否”时各取值条件概率的极大似然估计。

其他两个属性同理。

  

**7.2\* 试证明:条件独立性假设不成立时，朴素贝叶斯分类器仍有可能产生最优贝叶斯分类器.**

**答：**4



在朴素贝叶斯分类器中，条件独立性假设是指给定目标值时，各个特征之间相互独立。这是一个强假设，实际上在现实世界的数据中往往是不成立的。然而，即使这个假设不成立，朴素贝叶斯分类器仍然可以工作得很好，并且有时甚至可以产生最优的贝叶斯分类器。

这是因为朴素贝叶斯分类器的性能并不完全依赖于特征之间的独立性。它的主要优势在于对后验概率的估计，即 

P(Y∣X)，其中 Y 是类别变量，X是特征向量。朴素贝叶斯利用贝叶斯定理和特征的条件概率来计算后验概率。即使特征之间存在依赖关系，只要这种依赖不是分类决策的主要因素，朴素贝叶斯仍然可以近似地估计出正确的后验概率。

此外，即使条件独立性假设不成立，朴素贝叶斯分类器在某些情况下也可能产生最优贝叶斯分类器。这通常发生在以下情况：

1. 类别之间的边界非常清晰，即使特征之间存在依赖关系，这种依赖也不会对分类结果产生重大影响。
2. 特征之间的依赖关系在所有类别中都是一致的，这样即使这些特征不是完全独立的，朴素贝叶斯也能够正确地分类。

总的来说，朴素贝叶斯分类器的有效性并不完全取决于条件独立性假设，而是取决于它对后验概率的估计能力以及数据的特定特性。在实践中，即使条件独立性假设不成立，朴素贝叶斯分类器也经常被发现能够提供非常强的性能。

令一组数据中有 A,B,CA,B,CA,B,C 三中属性，其目标值为Bool型变量，即取值 +,−+,-+,- 且其概率相同，即 P(+)\=P(−)\=12P(+)=P(-)=\\frac{1}{2}P(+)=P(-)=\\frac{1}{2} 。给定一个样本 EEE ，令 P(A|+)P(A|+)P(A|+) 表示 P(A\=aE|+)P(A=a\_{E}|+)P(A=a\_{E}|+) ， aEa\_Ea\_E 表示 EEE 中属性 AAA 的取值。假设 A,CA,CA,C完全独立，而 A\=BA=BA=B 即两者完全相关，因此理论上对于最优贝叶斯分类器来说，属性 BBB 应该被忽略，那么基于最优贝叶斯分类器时，其决策准则（书中P7.15式）可描述为：若 P(+)P(A|+)P(C|+)−P(−)P(A|−)P(C|−)\=P(A|+)P(C|+)−P(A|−)P(C|−)\>0P(+)P(A|+)P(C|+)-P(-)P(A|-)P(C|-)=P(A|+)P(C|+)-P(A|-)P(C|-)>0P(+)P(A|+)P(C|+)-P(-)P(A|-)P(C|-)=P(A|+)P(C|+)-P(A|-)P(C|-)>0 时，则将样本 EEE 归为 +++ 类；而另一方面，考虑朴素贝叶斯分类器，在分类决策时会将属性 BBB 也考虑在内，此时相当于将 AAA 计算了两次，此时决策准则则描述为： P(A|+)2P(C|+)−P(A|−)2P(C|−)\>0P(A|+)^2P(C|+)-P(A|-)^2P(C|-)>0P(A|+)^2P(C|+)-P(A|-)^2P(C|-)>0 时，将 EEE 归为 +++ 类。

根据贝叶斯理论， P(A|+)\=P(A)P(+|A)/P(+)P(A|+) = P(A)P(+|A)/P(+)P(A|+) = P(A)P(+|A)/P(+) ，且由于 P(+)\=P(−)P(+)=P(-)P(+)=P(-) ，则最优贝叶斯分类可表示为： P(+|A)P(+|C)−P(−|A)P(−|C)\>0P(+|A)P(+|C)-P(-|A)P(-|C)>0P(+|A)P(+|C)-P(-|A)P(-|C)>0 ；而朴素贝叶斯则表示为： P(+|A)2P(+|C)−P(−|A)2P(−|C)\>0P(+|A)^2P(+|C)-P(-|A)^2P(-|C)>0P(+|A)^2P(+|C)-P(-|A)^2P(-|C)>0 。取 P(+|A)\=p,P(+|C)\=qP(+|A)=p,P(+|C)=qP(+|A)=p,P(+|C)=q ，于是最优贝叶斯分类器为： pq−(1−p)(1−q)\=p+q−1\>0⇒q\>1−ppq-(1-p)(1-q)=p+q-1>0\\Rightarrow q>1-ppq-(1-p)(1-q)=p+q-1>0\\Rightarrow q>1-p ，朴素贝叶斯为 p2q−(1−p)2(1−q)\>0⇒q\>(1−p)2p2+(1−p)2p^2q-(1-p)^2(1-q)>0\\Rightarrow q>\\frac{(1-p)^2}{p^2+(1-p)^2}p^2q-(1-p)^2(1-q)>0\\Rightarrow q>\\frac{(1-p)^2}{p^2+(1-p)^2} ，两者决策边界如下图：

<img src="https://pic1.zhimg.com/v2-3c1bd232483a289d420eae88fc7524ec\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="473" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic1.zhimg.com/v2-3c1bd232483a289d420eae88fc7524ec\_r.jpg"/>

![](https://pic1.zhimg.com/80/v2-3c1bd232483a289d420eae88fc7524ec_1440w.webp)

只有在两者决策边界之间（浅黄色区域），其分类情况是不同的，在其他区域，朴素贝叶斯分类结果和最优贝叶斯的分类结果是相同的，因此即便属性之间独立性假设不成立，朴素贝叶斯在某些条件（本例中就是属性概率分布在两者相交区域之外）下任然是最优贝叶斯分类器。

**参考：**

**《On the Optimality of the Simple Bayesian Classifier under Zero-One Loss》**

ps.这个例子就是来自该论文，只做了一点翻译工作。论文中给出了更全面的理论证明，和朴素贝叶斯产生最优贝叶斯分类的充分必要条件。本打算看完把理论证明也尝试复述一遍，但能力有限，一方面没有理解很透彻，另一方面证明过程有点长感觉表达能力有点不大够用...

  

**7.3 试编程实现拉普拉斯修正的朴素贝叶斯分类器，并以西瓜数据集 3.0 为训练集，对 p.151 "测1" 样本进行判别.**

**答：**

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch7--%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF%25E5%2588%2586%25E7%25B1%25BB/7.3)

这里代码很简单。不怎么规范。

  

**7.4 实践中使用式 (7.15)决定分类类别时，若数据的维数非常高，则概率连乘** $\prod_{i=1}^{d}P(x_i|c) $**的结果通常会非常接近于 0 从试述防止下溢的可能方案.而导致下溢.**

**答：**



在处理高维数据时，确实，使用朴素贝叶斯分类器中的概率连乘公式 $\prod_{i=1}^{d}P(x_i|c) $可能会导致数值下溢，因为连乘大量小于1的概率值会得到一个非常接近于0的结果。为了防止这种下溢，我们可以采用对数变换的方法。



具体来说，我们可以对每个概率值取自然对数，然后将乘法转换为加法。这是因为对数函数具有这样的性质：

$ln(a⋅b)=ln(a)+ln(b)$。因此，原来的连乘公式可以转换为：$ln(\prod_{i=1}^{d}P(x_i|c) )=\sum_{i=1}^{d}P(x_i|c) $，从而可以将（7.15）改为： $h_{nb}(x)=\mathop{\arg}max_{\theta}log(P(c))+\sum_{i=1}^{d}log(P(x\_i|c))$

这样，即使是非常小的概率值，在取了自然对数之后也不会导致下溢，因为加法不会像乘法那样放大小数的影响。最终，我们可以比较不同类别下的对数后验概率的和，而不是原始的概率连乘的结果，来决定分类类别。

这种方法不仅可以防止下溢，还可以提高计算效率，因为加法运算通常比乘法运算要快。此外，对数变换是单调的，这意味着它不会改变概率值的相对大小，因此不会影响分类决策。





这在p153中已经给出答案了。即取对数将连乘转化为“连加”防止下溢。

即将式（7.15）改为： h\_{nb}(x)=\\begin{equation} \\mathop{\\arg\\max}\_{\\theta}log(P(c))+\\sum\_{i=1}^{d}log(P(x\_i|c)) \\end{equation}h\_{nb}(x)=\\begin{equation} \\mathop{\\arg\\max}\_{\\theta}log(P(c))+\\sum\_{i=1}^{d}log(P(x\_i|c)) \\end{equation} 。

  

**7.5 试证明:二分类任务中两类数据满足高斯分布且方差相同时，线性判别分析产生贝叶斯最优分类器.**

**答：**

首先看一下贝叶斯最优分类器：在书中p148中解释了对于最小化分类错误率的贝叶斯最优分类器可表示为： h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(c|x) \\end{equation}h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(c|x) \\end{equation} ，由贝叶斯定理即转换为： h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(x|c)P(c) \\end{equation}h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(x|c)P(c) \\end{equation} 。那么在数据满足高斯分布时有： h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(x|c)P(c) \\end{equation} h^\*(x)=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}P(x|c)P(c) \\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}log(f(x|c)P(c) )\\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}log(f(x|c)P(c) )\\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}log(\\frac{1}{(2\\pi)^{n/2}\\left| \\Sigma \\right|^{1/2}}exp(-\\frac{1}{2}(x-\\mu\_c)^T \\Sigma^{-1}(x-\\mu\_c))) + log(P(c))\\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y}log(\\frac{1}{(2\\pi)^{n/2}\\left| \\Sigma \\right|^{1/2}}exp(-\\frac{1}{2}(x-\\mu\_c)^T \\Sigma^{-1}(x-\\mu\_c))) + log(P(c))\\end{equation}

\=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y} -\\frac{1}{2}(x-\\mu\_c)^T \\Sigma^{-1}(x-\\mu\_c) + log(P(c)) \\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y} -\\frac{1}{2}(x-\\mu\_c)^T \\Sigma^{-1}(x-\\mu\_c) + log(P(c)) \\end{equation}

\=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y} x^T\\Sigma^{-1}\\mu\_c - \\frac{1}{2}\\mu\_c^T\\Sigma^{-1}\\mu\_c + log(P(c)) \\end{equation} \=\\begin{equation} \\mathop{\\arg\\max}\_{c\\in y} x^T\\Sigma^{-1}\\mu\_c - \\frac{1}{2}\\mu\_c^T\\Sigma^{-1}\\mu\_c + log(P(c)) \\end{equation}

在二分类任务中，贝叶斯决策边界可表示为 g(x)= x^T\\Sigma^{-1}\\mu\_1 -x^T\\Sigma^{-1}\\mu\_0 - (\\frac{1}{2}\\mu\_1^T\\Sigma^{-1}\\mu\_1 - \\frac{1}{2}\\mu\_0^T\\Sigma^{-1}\\mu\_0) + log(\\frac{P(1)}{P(0)}) g(x)= x^T\\Sigma^{-1}\\mu\_1 -x^T\\Sigma^{-1}\\mu\_0 - (\\frac{1}{2}\\mu\_1^T\\Sigma^{-1}\\mu\_1 - \\frac{1}{2}\\mu\_0^T\\Sigma^{-1}\\mu\_0) + log(\\frac{P(1)}{P(0)})

\=x^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)- \\frac{1}{2}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0) +\\log(\\frac{P(1)}{P(0)})\=x^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)- \\frac{1}{2}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0) +\\log(\\frac{P(1)}{P(0)})

再看看线性判别分析：

书中p62给出式3.39，其投影界面可等效于 w=(\\Sigma\_0+\\Sigma\_1)^{-1}(\\mu\_1-\\mu\_0)w=(\\Sigma\_0+\\Sigma\_1)^{-1}(\\mu\_1-\\mu\_0) ，注意为了和上面的推导一致，这里和书中给出的差了一个负号，但 ww 位置没有改变，只是改变了方向而已。在两类别方差相同时有： w=\\frac{1}{2}\\Sigma^{-1}(\\mu\_1-\\mu\_0)w=\\frac{1}{2}\\Sigma^{-1}(\\mu\_1-\\mu\_0) ，两类别在投影面连线的中点可为 \\frac{1}{2}(\\mu\_1+\\mu\_0)^Tw=\\frac{1}{2}(\\mu\_1+\\mu\_0)^Tw= \\frac{1}{4}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)\\frac{1}{4}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0) ，那么线性判别分析的决策边界可表示为 g(x)=x^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)-\\frac{1}{2}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)g(x)=x^T\\Sigma^{-1}(\\mu\_1-\\mu\_0)-\\frac{1}{2}(\\mu\_1+\\mu\_0)^T\\Sigma^{-1}(\\mu\_1-\\mu\_0) 。

推导到这里发现贝叶斯最优分类器和线性判别分析的决策边界只相差 log(\\frac{P(1)}{P(0)})log(\\frac{P(1)}{P(0)}) ，在题目左边小字中有提及，“假设同先验”，所以 log(\\frac{P(1)}{P(0)}) = 0log(\\frac{P(1)}{P(0)}) = 0 ，于是得证。

  

**7.6 试编程实现 AODE 分类器，并以西瓜数据集 3.0 为训练集，对 p.151的"测1" 样本进行判别.**

**答：**

代码在：

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch7--%25E8%25B4%259D%25E5%258F%25B6%25E6%2596%25AF%25E5%2588%2586%25E7%25B1%25BB/7.6)

提一下关于连续值处理的问题。这个书中和原论文(好像)都没有提交，所以按照自己的理解来处理了。考虑到以下，实现过程中不将连续值作为父属性。

*   此外AODE本身就是要将取值样本数量低于一定阈值(论文中给出的是30)的属性去除的，从这个角度来说，连续值就不能作为父属性了，当前其实可以按照区间划分将连续值离散化。

另外，虽然在样本这么小的情况下，看预测结果实际意义不大，但相比于朴素贝叶斯，AODE对于西瓜数据集的拟合更好（错误率更低）。

ps.书中给出的式(7.24)有错误的，分母的N\_iN\_i 改正为 N\\times N\_iN\\times N\_i ，在第十次印刷的时候纠正了，看旧版书的同学要注意了。

  

**7.7 给定 d 个二值属性的二分类任务，假设对于任何先验概率项的估算至少需 30 个样例，则在朴素贝叶斯分类器式 (7.15) 中估算先验概率项 P(c)P(c) 需 30 x 2 = 60 个样例.试估计在 AODE 式 (7.23) 中估算先验概率项 P(c,x\_i)P(c,x\_i) 所需的样例数(分别考虑最好和最坏情形) .**

**答：**

这里“假设对于任何先验概率项的估算至少需 30 个样例”意味着在所有样本中， 任意c,x\_ic,x\_i 的组合至少出现30次。

当 d=1d=1 时，即只有一个特征 x\_1x\_1 ，因为是二值属性，假设取值为 1,01,0 ，那为了估计 p(y=1,x\_1=1)p(y=1,x\_1=1) 至少需要30个样本，同样 p(y=1,x\_1=0)p(y=1,x\_1=0) 需要额外30个样本，另外两种情况同理，所以在 d=1d=1 时，最好和最坏情况都需要120个样本。

再考虑 d=2d=2 ，多加个特征 x\_2x\_2 同样取值 1,01,0 ，为了满足求 P(c,x\_1)P(c,x\_1) 已经有了120个样本，且60个正样本和60个负样本；在最好的情况下，在60个正样本中，正好有30个样本 x\_2=1x\_2=1 ,30个 x\_2=0x\_2=0 ，负样本同理，此时这120个样本也同样满足计算 P(c,x\_2)P(c,x\_2) 的条件，所有 d=2d=2 时，最好的情况也只需要120个样本， d=nd=n 时同理；在最坏的情况下，120个样子中， x\_2x\_2 都取相同的值 11 ，那么为了估算 P(c,x\_2=0)P(c,x\_2=0) 需要额外60个样本，总计180个样本，同理计算出 d=2,3,4...d=2,3,4... 时的样本数，即每多一个特征，最坏情况需要多加额外60个样本， d=nd=n 时，需要 60(n+1)60(n+1) 个样本。

那么 dd 个二值属性下，最好情况需要120个样本，最好情况需要 60(d+1)60(d+1) 个样本。

  

<img src="https://pic1.zhimg.com/v2-b3870d803eff2224b7e50f589d7114d4\_b.png" data-caption="" data-size="normal" data-rawwidth="552" data-rawheight="50" class="origin\_image zh-lightbox-thumb" width="552" data-original="https://pic1.zhimg.com/v2-b3870d803eff2224b7e50f589d7114d4\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='552' height='50'></svg>)

**答**

**这个问题主要基于书中式7.26，就很容易理解了**

首先考虑同父结构，根据式7.26，其联合分布可以表示为： P(x\_1,x\_3,x\_4)=P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1)P(x\_1,x\_3,x\_4)=P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1) ，

在给定 x\_1x\_1 时， P(x\_3,x\_4|x\_1)=\\frac{P(x\_1,x\_3,x\_4)}{P(x\_1)}=\\frac{P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1)}{P(x\_1)}=P(x\_3|x\_1)P(x\_4|x\_1)P(x\_3,x\_4|x\_1)=\\frac{P(x\_1,x\_3,x\_4)}{P(x\_1)}=\\frac{P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1)}{P(x\_1)}=P(x\_3|x\_1)P(x\_4|x\_1)

即同父结构中 x\_3,x\_4x\_3,x\_4 关于 x\_1x\_1 条件独立；

在 x\_1x\_1 取值未知时有：P(x\_3,x\_4)=\\sum\_{x\_1}P(x\_1,x\_3,x\_4)=\\sum\_{x\_1}P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1)P(x\_3,x\_4)=\\sum\_{x\_1}P(x\_1,x\_3,x\_4)=\\sum\_{x\_1}P(x\_1)P(x\_3|x\_1)P(x\_4|x\_1) 是推不出 P(x\_3,x\_4)=P(x\_3)P(x\_4)P(x\_3,x\_4)=P(x\_3)P(x\_4) 的，所以同父结构中 x\_3,x\_4x\_3,x\_4 关于 x\_1x\_1 边际独立不成立。

  

再考虑顺序结构，其联合分布有： P(x,y,z)=P(z)P(x|z)P(y|x)P(x,y,z)=P(z)P(x|z)P(y|x)

给定 xx 时，

P(y,z|x)=\\frac{P(x,y,z)}{P(x)}=\\frac{P(z)P(x|z)P(y|x)}{P(x)}=\\frac{P(x,z)P(y|x)}{P(x)}=P(z|x)P(y|x) P(y,z|x)=\\frac{P(x,y,z)}{P(x)}=\\frac{P(z)P(x|z)P(y|x)}{P(x)}=\\frac{P(x,z)P(y|x)}{P(x)}=P(z|x)P(y|x)

，即顺序结构中 y,zy,z 关于 xx 条件独立；

在 xx 取值未知时有：

P(y,z)=\\sum\_xP(x, y, z)=\\sum\_xP(z)P(x|z)P(y|x)P(y,z)=\\sum\_xP(x, y, z)=\\sum\_xP(z)P(x|z)P(y|x) ，同样推不出 P(y,z)=P(y)P(z)P(y,z)=P(y)P(z) ，所以顺序结构中，同样 y,zy,z 关于 xx 边际独立不成立。

* * *

好久没更新...罪过，堕落了...前面八题一个月之前就写好了，一直在看7.7(主要还是懒.)阅读材料给出的贝叶斯网相关论文(主要是《A Tutorial on Learning With Bayesian Networks》)，下面两题应该还是需要写代码实现的，回头有时间再补把。

**7.9 以西瓜数据集 2.0 为训练集，试基于 BIC 准则构建一个贝叶斯网.**

**答：**

关于贝叶斯网结构的构建，书中p160只稍微提了一下，不过还是挺好理解的，《A Tutorial on Learning With Bayesian Networks》11节给出了更详细的描述。比较简单是方法就是贪心法：

*   1) 初始化一个网络结构；
*   2) 使用E表示当前合法的改变一条边的操作集合，比如若两个节点已经有连接，那么合法操作可以删除或者逆转，如没有连接则可以增加一条边，当前必须是在保证不会形成回路的情况；
*   3) 从中选择一个使得BIC下降最大的一个作为实际操作；
*   4) 循环2,3直到BIC不再下降。

论文中也给出了其他算法。

有时间再补代码吧。

  

**7.10 以西瓜数据集 2.0 中属性"脐部"为隐变量，试基于 EM 算法构建一个贝叶斯网.**

**答：**

**待填坑。**

本文转自 <https://zhuanlan.zhihu.com/p/51768750>，如有侵权，请联系删除。