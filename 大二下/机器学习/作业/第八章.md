**8.1 假设抛硬币正面朝上的概率为 p ， 反面朝上的概率为 1−p 。 令 H(n) 代表抛 n 次硬币所得正面朝上的次数，则最多 k 次正面朝上的概率为**

**P(H(n)≤k)\=∑i\=0k(ni)pi(1−p)n−iP(H(n)\\leq k)=\\sum\_{i=0}^{k}\\begin{pmatrix}n\\\\i\\end{pmatrix}p^{i}(1-p)^{n-i}P(H(n)\\leq k)=\\sum\_{i=0}^{k}\\begin{pmatrix}n\\\\i\\end{pmatrix}p^{i}(1-p)^{n-i} . (8.43)**

**对δ\>0,k\=(p−δ)n\\delta>0,k=(p-\\delta)n\\delta>0,k=(p-\\delta)n 有： HoeffdingHoeffdingHoeffding 不等式**

**P(H(n)≤(p−δ)n)≤e−2δ2nP(H(n)\\leq(p-\\delta)n)\\leq e^{-2\\delta^{2}n}P(H(n)\\leq(p-\\delta)n)\\leq e^{-2\\delta^{2}n} . (8.44)**

**试推导8.3。**

**答：**

8.3式如下：P(H(x)≠f(x))\=∑k\=0⌊T/2⌋(Tk)(1−ϵ)kϵT−k≤exp(−12T(1−2ϵ)2)P(H(x)\\ne f(x))=\\sum\_{k=0}^{\\lfloor T/2 \\rfloor}\\begin{pmatrix}T\\\\k\\end{pmatrix}(1-\\epsilon)^{k}\\epsilon^{T-k}\\leq exp(-\\frac{1}{2}T(1-2\\epsilon)^{2})P(H(x)\\ne f(x))=\\sum\_{k=0}^{\\lfloor T/2 \\rfloor}\\begin{pmatrix}T\\\\k\\end{pmatrix}(1-\\epsilon)^{k}\\epsilon^{T-k}\\leq exp(-\\frac{1}{2}T(1-2\\epsilon)^{2}) .第一个等式很好理解， (Tk)(1−ϵ)kϵT−k\\begin{pmatrix}T\\\\k\\end{pmatrix}(1-\\epsilon)^{k}\\epsilon^{T-k}\\begin{pmatrix}T\\\\k\\end{pmatrix}(1-\\epsilon)^{k}\\epsilon^{T-k} 表示 TTT 个基分类器中有 T−kT-kT-k 个分类器预测错误的概率， ∑k\=0⌊T/2⌋\\sum\_{k=0}^{\\lfloor T/2 \\rfloor}\\sum\_{k=0}^{\\lfloor T/2 \\rfloor} 即将所有 H(x)≠f(x)H(x)\\ne f(x)H(x)\\ne f(x) 下可能预测错误的基分类器个数遍历取总。或者可以直接套用8.43式， P(H(x)≠f(x))\=P(H(T)≤⌊T/2⌋)P(H(x)\\ne f(x))=P(H(T)\\leq\\lfloor T/2 \\rfloor)P(H(x)\\ne f(x))=P(H(T)\\leq\\lfloor T/2 \\rfloor) ，其中 H(T)H(T)H(T) 表示 TTT 个基分类器中，预测正确的个数，即可得第一个等式。

关于第二个不等式，

取8.44中 δ\=1−ϵ−⌊T/2⌋T\\delta=1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T}\\delta=1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T}， ⌊T/2⌋T≤12\\frac{\\lfloor T/2 \\rfloor}{T}\\leq \\frac{1}{2}\\frac{\\lfloor T/2 \\rfloor}{T}\\leq \\frac{1}{2} 且于是：P(H(T)≤⌊T/2⌋)\=P(H(T)≤(1−ϵ−δ)T)≤e−2(1−ϵ−⌊T/2⌋T)2T≤e−12T(1−2ϵ)2P(H(T)\\leq\\lfloor T/2 \\rfloor)=P(H(T)\\leq(1-\\epsilon-\\delta)T)\\leq e^{-2(1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T})^{2}T}\\leq e^{-\\frac{1}{2}T(1-2\\epsilon)^{2}}P(H(T)\\leq\\lfloor T/2 \\rfloor)=P(H(T)\\leq(1-\\epsilon-\\delta)T)\\leq e^{-2(1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T})^{2}T}\\leq e^{-\\frac{1}{2}T(1-2\\epsilon)^{2}}于是8.3得证。这里需要注意一点 e−2(1−ϵ−⌊T/2⌋T)2T≤e−12T(1−2ϵ)2e^{-2(1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T})^{2}T}\\leq e^{-\\frac{1}{2}T(1-2\\epsilon)^{2}}e^{-2(1-\\epsilon-\\frac{\\lfloor T/2 \\rfloor}{T})^{2}T}\\leq e^{-\\frac{1}{2}T(1-2\\epsilon)^{2}} 是建立在 ϵ<12\\epsilon<\\frac{1}{2}\\epsilon<\\frac{1}{2} 的基础上的。

  

**8.2 对于 0/10/10/1 损失函数来说，指数损失函数并非仅有的一致替代函数。考虑式 (8.5) ，试证明：任意损失函数 ℓ(−f(x)H(x))\\ell(-f(x)H(x))\\ell(-f(x)H(x)) ， 若对于 H(x)H(x)H(x) 在区间 \[−∞,δ\](δ\>0)\[-\\infty,\\delta\](\\delta>0)\[-\\infty,\\delta\](\\delta>0) 上单调递减，则 ℓ\\ell\\ell 是 0/10/10/1 损失函数的一致替代函数。**

**答：**

暂时没想出怎么证明。

  

**8.3 从网上下载或自己编程实现 AdaBoost，以不剪枝抉策树为基学习器，在西瓜数据集 3.0α 上训练一个 AdaBoost 集成，并与图 8.4进行比较。**

**答：**

代码在：[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch8--%25E9%259B%2586%25E6%2588%2590%25E5%25AD%25A6%25E4%25B9%25A0)

这里题目有点问题，如果以不剪枝决策树为基学习器，可以生成一个完美符合数据的决策树，此时AdaBoost就没意义了，因为第一颗树错误率就为0了，样本权重也不会发生改变。

所有这里代码是限定树的深度为2，但训练到第四颗树，错误率就已经到0了，下图给出来的决策边界，其实基本上就是第四颗树的决策了，因为错误率为0，其权重太大。

<img src="https://pic4.zhimg.com/v2-cf33410d61db0f02b750e1daf3d5fc03\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="476" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic4.zhimg.com/v2-cf33410d61db0f02b750e1daf3d5fc03\_r.jpg"/>

![](https://pic4.zhimg.com/80/v2-cf33410d61db0f02b750e1daf3d5fc03_1440w.webp)

**8.4 GradientBoosting \[Friedman, 2001\] 是一种常用的 Boosting 算法，试析其与 AdaBoost 的异同.**

**答：**

> GradientBoosting 和 AdaBoost 都是 Boosting 算法的变体，它们通过组合多个弱学习器来创建一个强学习器。不过，它们在细节上有一些不同：
>
> **GradientBoosting**:
>
> - **基于梯度下降**：GradientBoosting 通过最小化损失函数来优化模型，通常使用均方误差（MSE）或对数损失。
> - **连续添加弱学习器**：每个新的弱学习器都是为了预测前一个模型的残差错误。
> - **使用收缩率**：为了避免过拟合，GradientBoosting 在添加新的弱学习器时会应用收缩率（也称为学习率）。
>
> **AdaBoost**:
>
> - **权重调整**：AdaBoost 通过增加之前分类错误的样本权重来调整弱学习器的关注点。
> - **不直接最小化损失函数**：AdaBoost 关注于提高分类错误的样本的权重，而不是直接最小化一个损失函数。
> - **不使用收缩率**：AdaBoost 在添加新的弱学习器时不会使用收缩率。
>
> 总的来说，GradientBoosting 通过最小化损失函数来优化模型，而 AdaBoost 通过调整样本权重来提高弱学习器的性能。GradientBoosting 使用了收缩率来控制模型复杂度，而 AdaBoost 则没有这一机制。
>
> 

这个问题，网上已经有很多总结了：

> Gradient Boosting和其它Boosting算法一样，通过将表现一般的数个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。抽象地说，模型的训练过程是对一任意可导目标函数的优化过程。通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。因此可以说Gradient Boosting = Gradient Descent + Boosting。  
> 和AdaBoost一样，Gradient Boosting也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足而Gradient Boosting是通过算梯度（gradient）来定位模型的不足。因此相比AdaBoost, Gradient Boosting可以使用更多种类的目标函数。

参考：[机器学习算法中GBDT与Adaboost的区别与联系是什么？](https://www.zhihu.com/question/54626685)

  

**8.5 试编程实现 Bagging，以决策树桩为基学习器，在西瓜数据集 3.0α 上训练一个 Bagging 集戚，井与图 8.6 进行比较.**

**答：**

**代码在：**[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch8--%25E9%259B%2586%25E6%2588%2590%25E5%25AD%25A6%25E4%25B9%25A0)

以决策树桩作为Bagging的基学习器，效果不太好。尝试了下，设置基学习器数量为21时算是拟合最好的，决策边界如下：

<img src="https://pic2.zhimg.com/v2-527e26a37c6ca6dbcc147449d72c16e1\_b.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="476" class="origin\_image zh-lightbox-thumb" width="640" data-original="https://pic2.zhimg.com/v2-527e26a37c6ca6dbcc147449d72c16e1\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='640' height='476'></svg>)

**8.6 试析 Bagging 通常为何难以提升朴素贝叶斯分类器的性能.**

**答：**



> 1. **朴素贝叶斯的偏差和方差**：朴素贝叶斯分类器通常具有较高的偏差和较低的方差。Bagging主要是通过降低模型的方差来提升性能的，但由于朴素贝叶斯的方差本身就不高，因此Bagging在这种情况下提升的空间有限。
> 2. **样本的代表性**：朴素贝叶斯分类器依赖于所有训练数据来估计概率分布，而Bagging通过从原始数据集中随机有放回地抽取子集来训练多个模型。这种抽样可能会导致每个子模型训练集的代表性不足，从而影响朴素贝叶斯分类器的性能。
> 3. **特征独立性假设**：朴素贝叶斯分类器的核心假设是特征之间相互独立。这个简化的模型假设导致其主要的误差来源是偏差而非方差。由于Bagging主要通过减少方差来提升模型性能，因此对于偏差主导的朴素贝叶斯分类器来说，Bagging的效果有限。

书中P177和P179提到过：

> 从偏差—方差分解的角度看， Boosting 主要关住降低偏差，因此 Boosting能基于泛化性能相当弱的学习器构建出很强的集成.  
> 从偏差—方差分解的角度看， Bagging 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显.

朴素贝叶斯中假设各特征相互独立，已经是很简化模型，其误差主要是在于偏差，没有方差可降。

ps.同样道理，这也是为什么8.5中，以决策树桩为基学习器的Bagging时，效果很差的原因；决策树桩同样是高偏差低方差的模型。

个人理解：

*   方差大（偏差低）的模型往往是因为对训练数据拟合得过好，模型比较复杂，输入数据的一点点变动都会导致输出结果有较大的差异，它描述的是模型输出的预测值相比于真实值的离散程度，方差越大，越离散，所以为什么Bagging适合以不剪枝决策树、神经网络这些容易过拟合的模型为基学习器；
*   偏差大（方差低）的模型则相反，往往因为对训练数据拟合得不够，模型比较简单，输入数据发生变化并不会导致输出结果有多大改变，它描述的是预测值和和真实值直接的差距，偏差越大，越偏离真实值。

  

**8.7 试析随机森林为何比决策树 Bagging 集成的训练速度更快.**

**答：**

决策树的生成过程中，最耗时的就是搜寻最优切分属性；随机森林在决策树训练过程中引入了随机属性选择，大大减少了此过程的计算量；因而随机森林比普通决策树Bagging训练速度要快。

> 随机森林（Random Forest）相比于基于决策树的Bagging集成算法在训练速度上通常更快，主要原因包括：
>
> 1. **属性子集的随机选择**：在构建每棵树时，随机森林不是考虑所有的属性，而是从所有可用的属性中随机选择一个子集，并仅从这个子集中选择最优的属性进行分裂。这减少了每个节点在选择划分属性时的计算量
> 2. **决策树的深度控制**：随机森林中的决策树通常不会完全生长，即它们的深度被限制在一个较低的水平。相比之下，Bagging集成中的决策树可能会完全生长，导致更深的树和更多的节点。更浅的树意味着更少的分裂，从而加快了训练速度
> 3. **并行化**：随机森林的另一个优势是它易于并行化。由于每棵树的构建是相互独立的，因此可以在不同的处理器上同时构建多棵树，进一步提高训练速度
>
> 这些因素共同作用，使得随机森林在训练时比传统的基于决策树的Bagging集成更加高效。

**8.8 MultiBoosting 算法 \[Webb，2000\] 将 AdaBoost 作为 Bagging 的基学习器， Iterative Bagging 算法 \[Breiman，2001b\] 则是将 Bagging 作为AdaBoost 的基学习器.试比较二者的优缺点.**

**答：**

> MultiBoosting算法和Iterative Bagging算法都是集成学习方法，它们通过结合多个基学习器来提高模型的性能。下面是这两种算法的优缺点比较：
>
> **MultiBoosting算法**的优点在于：
>
> - 它结合了Bagging、Wagging和AdaBoost的特点，能有效降低误差和方差，尤其是误差。
> - 由于采用了AdaBoost作为基学习器，它能够关注并改善那些被前一个基学习器错误分类的样本。
>
> 然而，MultiBoosting算法的缺点包括：
>
> - 训练成本和预测成本都相对较高，因为需要训练多个AdaBoost模型，每个模型又包含多个弱学习器。
>
> **Iterative Bagging算法**的优点包括：
>
> - 相比单纯的Bagging算法，Iterative Bagging能够降低误差。
>
> 但是，Iterative Bagging算法的缺点是：
>
> - 方差可能会上升，因为它在每次迭代中都会调整样本权重，这可能导致模型变得更加复杂，从而增加方差。
> - 由于Bagging本身是一种降低方差的算法，Iterative Bagging可以被看作是Bagging与单分类器之间的折中。
>
> 总的来说，MultiBoosting和Iterative Bagging都试图通过不同的方式改善基学习器的性能，但它们在降低误差和方差方面的策略有所不同，且都会增加模型的计算成本。选择使用哪种算法通常取决于具体问题的需求以及可用资源。

  

**8.9\* 试设计一种可视的多样性度量，对习题 8.3 和习题 8.5 中得到的集成进行评估，并与 κ-误差圈比较.**

**答：**

待补。

  

**8.10\* 试设计一种能提升 k 近邻分类器性能的集成学习算法.**

**答：**

待补。

本文转自 <https://zhuanlan.zhihu.com/p/51206123>，如有侵权，请联系删除。