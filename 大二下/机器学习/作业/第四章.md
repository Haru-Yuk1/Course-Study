**4.1 试证明对于不含冲突数据(即特征向量完全相同但标记不同)的训练集，必存在与训练集一致(即训练误差为 0) 的决策树。**

**答：**

从原书p74的图4.2的决策树学习的基本算法可以看出，生成一个叶节点有三种情况：

1、节点下样本 DDD 全属于同一类样本 CCC ，则将当前节点作为 CCC 类叶节点。

2、属性集 A\=⊘ ，或者样本在当前属性集上取值相同。即特征用完了（当只剩最后一个特征时，进一步分裂，只能将各取值设立叶节点，标记为样本最多的类别。），或者的样本在 AAA 上取值都相同（感觉这里貌似和第 一条重复了）。这时取 DDD 中最多的类作为此节点的类别标记。

3、在某一节点上的属性值 a∗va\_{\*}^{v}a\_{\*}^{v} ，样本为空，即没有样本在属性 a∗a\_{\*}a\_{\*} 上取值为 a∗va\_{\*}^{v}a\_{\*}^{v} 。同样取 DDD 中最多的类作为此节点的类别标记。

在这道题中，目标是找出和训练集一致的决策树，所以不必考虑第3点，从1、2情况来看出决策树中树枝停止“生长”生成叶节点只会在样本属于同一类或者所有特征值都用完的时候，那么可能导致叶节点标记与实际训练集不同时只会发生在特征值都用完的情况（同一节点中的样本，其路径上的特征值都是完全相同的），而由于训练集中没有冲突数据，那每个节点上训练误差都为0。

  

具体的证明过程如下：

1. 如果当前节点包含的样本全属于同一类别，那么该节点就是一个叶节点，标记为该类别。
2. 如果当前属性集为空，或者所有样本在当前属性集上取值相同，那么也将该节点标记为叶节点，并将其标记为样本数最多的类别。
3. 如果当前节点包含的样本属于不同类别，且当前属性集非空，那么选择一个最优属性进行分裂，创建子节点，并对每个子节点递归地重复上述过程。

由于训练集中不存在冲突数据，即不存在两个样本在所有属性上取值相同但类别不同，因此在决策树的构建过程中，每个叶节点都会精确地对应到训练集中的一类样本，从而保证训练误差为0。

这样，我们就可以保证存在一个与训练集一致的决策树，即训练误差为0的决策树。这个过程也说明了决策树学习算法的完备性，即在给定足够大的假设空间（例如，允许决策树足够深），决策树算法能够对任何训练集构造一个与之一致的假设。当然，这并不意味着这样的决策树具有良好的泛化能力，因为它可能会过拟合训练数据。



**4.2 试析使用"最小训练误差"作为决策树划分选择准则的缺陷。**

**答：**

这道题暂时没想出答案。在网上找了其他的答案，都是认为会造成过拟合，没给出具体证明。而我的理解决策树本身就是容易过拟合的，就算使用信息增益或者基尼指数等，依旧容易过拟合，至于使用“最小训练误差”会不会“更容易”过拟合暂时没理解明白。

待填坑。

  具体来说，以下是一些关于使用最小训练误差作为划分选择准则的缺陷的分析：

1. **捕捉噪声**：如果数据集中包含噪声，最小训练误差准则可能会使决策树过于复杂，因为它试图创建一个完美拟合训练数据的模型，包括其中的噪声和异常点。
2. **缺乏泛化能力**：当模型对训练数据过度拟合时，它在新的、未见过的数据上的表现通常会下降，因为模型学习了训练数据中特定的特征，而这些特征可能不适用于其他数据集。
3. **简化模型的必要性**：为了提高模型的泛化能力，通常需要对模型进行简化，例如通过剪枝来减少模型复杂度。这与最小训练误差的目标相矛盾，后者倾向于创建更复杂的模型。
4. **评估指标的选择**：在实际应用中，除了训练误差之外，还有其他评估指标可以用来指导决策树的划分，如信息增益、基尼指数等。这些指标可以在一定程度上减轻过拟合的问题，并提供更好的泛化能力。

总的来说，虽然最小训练误差是一个直观的优化目标，但在构建决策树时，需要考虑其他因素，以确保模型具有良好的泛化能力。在选择划分准则时，应该考虑到模型的复杂度和泛化能力之间的平衡。

**4.3 试编程实现基于信息熵进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树。**

**答：**

因为数据集的原因，数据量比较小，在选择划分属性的时候会出现特征的信息增益或者信息增益率相同的情况。所有生成的决策树和书中可能不一致。并且在生成叶节点时，会出现两类数量一直的情况，这时候叶节点就随机设置一个分类了。

代码实现了以信息增益、增益率、基尼指数划分准则。下面一道题（4.4）也是用相同的代码。另外画图的代码是主要参考《机器学习实战》决策树那一章画图源码。

有些地方代码有点乱，比如进行剪枝的部分就有大量重复代码；并且预剪枝部分可以在生成决策树的时候实现，减少计算量。以后有机会再优化一下。

代码在：[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%25E5%2586%25B3%25E7%25AD%2596%25E6%25A0%2591/4.3-4.4)

生成决策树如下：

<img src="https://pic1.zhimg.com/v2-7f8bb9b74ee078cef89a70d972f5b3f8\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="924" class="origin\_image zh-lightbox-thumb" width="1920" data-original="https://pic1.zhimg.com/v2-7f8bb9b74ee078cef89a70d972f5b3f8\_r.jpg"/>

![](https://pic1.zhimg.com/80/v2-7f8bb9b74ee078cef89a70d972f5b3f8_1440w.webp)

**4.4 试编程实现基于基尼指数进行划分选择的决策树算法，为表 4.2 中数据生成预剪枝、后剪枝决策树并与未剪枝决策树进行比较.**

**答：**

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%25E5%2586%25B3%25E7%25AD%2596%25E6%25A0%2591/4.3-4.4)

未剪枝、后剪枝、预剪枝生成决策树分别如下，总体来说后剪枝会相比于预剪枝保留更多的分支。

有两个需要注意的地方。一个是在4.3中说过的，因为划分属性的信息增益或者基尼指数相同的原因，这个时候选择哪一个属性作为划分属性都是对的，生成决策树和书中不一致是正常的（书中第一个节点为“脐部”）。另外数据量这么小的情况下，常常会出现剪枝前后准确率不变的情况，原书中也提到这种情况通常要进行剪枝的，但是这道题中若进行剪枝，会出现只有一个叶节点的情况。为了画图好看点...所以都不无论在预剪枝还是后剪枝中，这种情况都会采取不剪枝策略。参考原书P82。

经过测试，在未剪枝的情况下，验证集上准确率为0.2857；后剪枝准确率为0.5714；预剪枝也为0.5714。

<img src="https://pic4.zhimg.com/v2-91de4f23c7b588b695d6332f44ca4353\_b.jpg" data-size="normal" data-rawwidth="1920" data-rawheight="924" class="origin\_image zh-lightbox-thumb" width="1920" data-original="https://pic4.zhimg.com/v2-91de4f23c7b588b695d6332f44ca4353\_r.jpg"/>

![](https://pic4.zhimg.com/80/v2-91de4f23c7b588b695d6332f44ca4353_1440w.webp)

未剪枝

<img src="https://pic4.zhimg.com/v2-05bbcbadbbb55f8f5827e1091c457597\_b.jpg" data-size="normal" data-rawwidth="1920" data-rawheight="924" class="origin\_image zh-lightbox-thumb" width="1920" data-original="https://pic4.zhimg.com/v2-05bbcbadbbb55f8f5827e1091c457597\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1920' height='924'></svg>)

后剪枝

<img src="https://pic1.zhimg.com/v2-a3985d803f50db2c3b3643c83e9b24cc\_b.jpg" data-size="normal" data-rawwidth="1920" data-rawheight="924" class="origin\_image zh-lightbox-thumb" width="1920" data-original="https://pic1.zhimg.com/v2-a3985d803f50db2c3b3643c83e9b24cc\_r.jpg"/>

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1920' height='924'></svg>)

预剪枝

  

**4\. 5 试编程实现基于对率回归进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树.**

**答：**

**这个没实现。**一种思路就是拟合对率回归后，从所有特征中选择一个 ww 值最高的一个特征值，即权重最高的一个特征值作为划分选择，但是没想好对于One-hot之后的特征权重怎么计算，比如“色泽”有三种取值“乌黑”、“青绿”、“浅白”，在One-hot之后会有三个特征，那么最后“色泽”这个特征的权重应该是取平均值？以后有机会....也不填坑。

  

**4.6 试选择 4 个 UCI 数据集，对上述 3 种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验.**

**答：**

只拿sklearn中自带的iris数据集试了一下剪枝后的准确率，发现不同随机数种子（使得数据集划分不同）导致最后验证集的准确率变化挺大。

**统计显著性检验没实现。**

[han1057578619/MachineLearning\_Zhouzhihua\_ProblemSets](https://link.zhihu.com/?target=https%3A//github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%25E5%2586%25B3%25E7%25AD%2596%25E6%25A0%2591/4.6)

  

**4.7 图 4.2 是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致"栈"溢出。试使用"队列"数据结构，以参数MaxDepth 控制树的最大深度，写出与图 4.2 等价、但不使用递归的决策树生成算法.**

**答：**

主要思路每一次循环遍历一层下节点(除去叶节点)，为每一个节点生成子节点，将非叶节点入队；用参数L保存每一层有多少个节点。下一次循环执行同样的步骤。直至所有的节点都叶节点，此时队列为空。具体如下：

```text
输入：训练集D = {(x1, y1), (x2, y2)...(xm, ym)};
	  属性集A = {a1, a2... ad};
	  最大深度MaxDepth = maxDepth
过程：函数TreeDenerate(D, A, maxDepth)
1:  生成三个队列，NodeQueue、DataQueue、AQueue分别保存节点、数据、和剩余属性集;
2:  生成节点Node_root;
3:  if A为空 OR D上样本都为同一类别:
4:		将Node_root标记为叶节点，其标记类别为D中样本最多的类;
5: 		return Node_root;
6:  end if 
7:  将Node入队NodeQueue; 将D入队 DataQueue; 将A入队AQueue;
8:  初始化深度depth=0;
9:  初始化L = 1;	# L用于记录每一层有多少非叶节点。
10: while NodeQueue 非空:
11:		L* = 0
12:		for _ in range(L):			# 遍历当前L个非叶节点
13:			NodeQueue 出队Node; DataQueue出队D; AQueue 出队A;
14:			从A中选择一个最优划分属性a*;
15:			for a* 的每一个值 a*v do:
16:				新建一个node*，并将node*连接为Node的一个分支;
17:				令 Dv表示为D中在a*上取值为a*v的样本子集;
18:				if Dv为空:
19:					将node*标记为叶节点，其标记类别为D中样本最多的类;
20:					continue;
21:				end if
22:				if A\{a*}为空 OR Dv上样本都为同一类别 OR depth == maxDepth:
23:					将node*标记为叶节点，其标记类别为Dv中样本最多的类;
24:					continue;
25:				end if 			
26:				将node*入队NodeQueue; 将Dv入队 DataQueue; 将A\{a*} 入队AQueue;
27:				L* += 1;		# 用于计算在第depth+1 层有多少个非叶节点
28:		L = L*;
29:		depth += 1;
30:	return Node_root;
输入以Node_root为根节点的一颗决策树
```

  

**4.8 试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode控制树的最大结点数，将题 4.7 中基于队列的决策树算法进行改写。对比题 4.7 中的算法，试析哪种方式更易于控制决策树所需存储不超出内存。**

**答：**

4.7写的算法就是广度优先搜索的。这道题将MaxNode改为MaxDepth，只需要改几个地方。有一点需要注意的地方，就是在给一个节点生成子节点时（19-32行），可能造成节点数大于最大值的情况，比如某属性下有3种取值，那么至少要生成3个叶节点，这个时候节点总数可能会超过最大值，这时最终节点数可能会是MaxNode+2。

至于两种算法对比。个人理解当数据特征值，各属性的取值较多时，形成的决策树会趋于较宽类型的树，这时使用广度优先搜索更容易控制内存。若属性取值较少时，深度优先搜索更容易控制内存。

对4.7中修改如下：

```text
输入：训练集D = {(x1, y1), (x2, y2)...(xm, ym)};
	  属性集A = {a1, a2... ad};
	  最大深度MaxNode = maxNode
过程：函数TreeDenerate(D, A, maxNode)
1:  生成三个队列，NodeQueue、DataQueue、AQueue分别保存节点、数据、和剩余属性集;
2:  生成节点Node_root;
3:  if A为空 OR D上样本都为同一类别:
4:		将Node_root标记为叶节点，其标记类别为D中样本最多的类;
5: 		return Node_root;
6:  end if 
7:  将Node入队NodeQueue; 将D入队 DataQueue; 将A入队AQueue;
8:  初始化深度numNode=1;
9:  初始化L = 1;	# L用于记录每一层有多少非叶节点。
10: while NodeQueue 非空:
11:		L* = 0
12:		for _ in range(L):			# 遍历当前L个非叶节点
13:			NodeQueue 出队Node; DataQueue出队D; AQueue 出队A;
14:			if numNode >= maxNode:
15:				将Node标记为叶节点，其标记类别为D中样本最多的类;
16:				continue;
17:			end if;
18:			从A中选择一个最优划分属性a*;
19:			for a* 的每一个值 a*v do:
20:				numNode+=1
21:				生成一个node*，并将node*连接为Node的一个分支;
22:				令 Dv表示为D中在a*上取值为a*v的样本子集;
23:				if Dv为空:
24:					将node*标记为叶节点，其标记类别为D中样本最多的类;
25:					continue;
26:				end if
27:				if A\{a*}为空 OR Dv上样本都为同一类别:
28:					将node*标记为叶节点，其标记类别为Dv中样本最多的类;
29:					continue;
30:				end if 			
31:				将node*入队NodeQueue; 将Dv入队 DataQueue; 将A\{a*} 入队AQueue;
32:				L* += 1;		# 用于计算在第depth+1 层有多少个非叶节点
33:			end if;
34:		L = L*;
35:	return Node_root;
```

  

  

**4.9 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去.**

**答：**

这道题相对简单。使用书中式4.9、4.10、4.11，对于原书中4.5式可以推广为：

$$Gini(D) =1-\sum_{k=1}^{\left| y \right|}\tilde{p_{k}}^{2} $$，属性a的基尼指数可推广为：

$$Gini\_index(D, a)=p\times Gini\_index(\tilde{D}, a) =p\times\sum_{v=1}^{V}\tilde{v}Gini(D^{v}) $$。

  

**4.10 从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集 3.0 上产生的结果**

**答：**

待补充。

本文转自 <https://zhuanlan.zhihu.com/p/44666694/?utm_id=0>，如有侵权，请联系删除。